import sys
import math 
import csv
import random as rn
import numpy as np
import hw2 as h2

"""
Generalization Error
Questions 1-3

import sys
sys.path.append('/home/jbkim/git/edx/cs1156x/week4')
sys.path.append('/home/jbkim/git/edx/cs1156x/week2')
import hw4 as h


Use simple approximate bound m_H(N) = N^{d_vc}

1. d_vc = 10, delta = 0.05, epsilon = 0.05
    >>> h.converge(h.sample_complexity, 10)
    452956.86430197326

    [d] 460,000 

2. generalization error epsilon, with fixed values
   d_vc = 50, delta = 0.05, for N = 10000
   which one has the smallest epsilon for large N = 10000
    >>> f = lambda(x): h.bound_a(x, N=10000.0); h.converge(f, 0.01, 0.001)
    0.632174915200836
    >>> f = lambda(x): h.bound_b(x, N=10000.0); h.converge(f, 0.01, 0.001)
    0.3313087859616395
    >>> f = lambda(x): h.bound_c(x, N=10000.0); h.converge(f, 0.01, 0.001)
    0.2236982509669556
    >>> f = lambda(x): h.bound_d(x, N=10000.0); h.converge(f, 0.01, 0.001)
    0.21522797214546197

    [d] Devroye

3. for small N, say N-5 which bound is smallest?
    >>> f = lambda(x): h.bound_a(x, N=5.0); h.converge(f, 0.01, 0.001)
    13.828161484991483
    >>> f = lambda(x): h.bound_b(x, N=5.0); h.converge(f, 0.01, 0.001)
    7.048776564183685
    >>> f = lambda(x): h.bound_c(x, N=5.0); h.converge(f, 0.01, 0.001)
    5.101349698523652
    >>> f = lambda(x): h.bound_d(x, N=5.0); h.converge(f, 0.01, 0.001)
    5.592362776032442

    [c] Parrondo and Van den Broek
"""

def sample_complexity(N, d_vc=10, delta=0.05, epsilon=0.05):
  Nnew = (8/epsilon**2) * math.log( (4*(2.0*N)**d_vc) / delta )
  return Nnew

"""
  stupid iterative method
  >>> h.converge(h.sample_complexity,10)
  iteration 1 curr: 407633.530925
  iteration 2 curr: 449583.157928
  iteration 3 curr: 452717.630744
  iteration 4 curr: 452939.959118
  iteration 5 curr: 452955.670372
  iteration 6 curr: 452956.780346
  iteration 7 curr: 452956.858762
  452956.8587621032
"""
def converge( func, initial, tol = 0.01, *args ):
  prev = initial
  curr = func(prev)
  i = 0
  while( abs(prev - curr) > tol ):
    prev = curr
    curr = func(curr)  
    i += 1
    if(i%10==0):
      print "iteration %d curr: %f" % (i, curr)
  return curr

# NOTE: becareful of interger and float! you need things calculated in floats!!
def mH(N,d_vc):
  return (1.0*N)**d_vc;

# 0.632174915200836
def bound_a(epsilon, N=10000.0, d_vc=50, delta=0.05):
  return math.sqrt((8/N) * math.log( (4*mH(2*N,d_vc)) / delta ))
# same as above
def bound_a2(epsilon, N=10000.0, d_vc=50, delta=0.05):
  return math.sqrt((8/N) * (math.log(4) + d_vc*math.log(2*N) - math.log(delta)))

# 0.3313087859616395
def bound_b(epsilon, N=10000.0, d_vc=50, delta=0.05):
  a = math.sqrt(2*math.log(2*N*mH(N,d_vc)) / N)
  b = math.sqrt((2/N)*math.log(1/delta))
  c = 1/N
  return a+b+c

# 0.2236982936616912
def bound_c(epsilon, N=10000.0, d_vc=50, delta=0.05):
  return math.sqrt( (1/N)*(2*epsilon + math.log(6*mH(2*N,d_vc)/delta)) )

# 0.21522797214546197
def bound_d(epsilon, N=10000.0, d_vc=50, delta=0.05):
  # damn. caused numeric overflow because (N**2)**d_vc
  # return math.sqrt( (1/(2*N))*(4*epsilon*(1+epsilon) + math.log(4*mH(N**2,d_vc)/delta)) )
  return math.sqrt( (1/(2*N))*(4*epsilon*(1+epsilon) + (math.log(4)+(d_vc*2)*math.log(N)-math.log(delta))) )

def run2(n=100):
  result = []
  for i in range(3,10000):
    fa = lambda(x): bound_a(x, N=1.0*i)
    fb = lambda(x): bound_b(x, N=1.0*i)
    fc = lambda(x): bound_c(x, N=1.0*i)
    fd = lambda(x): bound_d(x, N=1.0*i)
    row = [converge(fa, i), converge(fb, i), converge(fc, i), converge(fd, i)]
    result.append(row)
  return result

"""
Once you save the csv, generate the plots by doing this R:

> t = read.csv('run2.csv', header=F,col.names=c('a','b','c','d'))
> names(t)
[1] "a" "b" "c" "d"
> dim(t)
[1] 9997    4
> png(file='error_bound_by_N.png',width=1000,height=600)
> matplot(t, type='l', col=c('black','red','blue','green'))
> dev.off()
null device
          1
"""
def savecsv(result, csvfile='./run2.csv'):
  with open(csvfile, 'wb') as f:
    writer = csv.writer(f)
    writer.writerows(result)


"""
Bias and Variance
Questions 4-6

f: [-1,1], given by f(x) = sin(PI*x)
input prob distro in uniform on [-1,1]

var(x)  = Ex[ Ed[(gD(x) - gbar(x))^2] ]
bias(x) = Ex[ (gbar(x) - f(x))^2 ]

  each gD(x) is the prediction generated by the hypothesis at each iteration
  gbar(x) is the average prediction from many hypotheses    
  f(x) is the target function sin(PI*x)

4. all hypotheses of the form h(x) = a*x
   E[g(x)] = gbar(x)   : expected value of hypothesis
   
   coefficients are given to second decimal accuracy
  
   You can't return simple slope = (y1-y0)/(x1-x0) of the points
   because the line has to pass through the origin .

    >>> h.q4(100000)
    1.4316070065314364

    [d] gbar(x) = 1.58x  WRONG!

    is it then
    [e] none of the above

    forum suggests 1.43 is not wrong

5. What is the closest value to the bias in this case?
    >>> h.q5(10000,genfunc=h.genxy_nointercept)
    {'bias_in': 0.26983915226025612, 'bias_out': 1.1942679404298699}
    
    [b] 0.3  *note this is the same as in-sample bias

6. What is the closest value to the variance in this case?
    >>> h.q6(5000,genfunc=h.genxy_nointercept)
    {'var_in': 0.19631055691704452, 'var_out': 0.23241018385835568}

    [a] 0.2

7. Which of the following learning models have least E_out ?
    >>> d = h.q7(100)
    >>> d['Eavg']
    array([   0.77624565,    0.47285492,    2.26368714,    8.60445837, 357.17422824])
    
    [b] h(x) = ax  (really??)

"""

def getf(x):
  return math.sin(math.pi * x)

def genxy_nointercept(n=2):
  x = np.array([rn.uniform(-1,1) for i in range(n)])
  y = np.array([getf(x1) for x1 in x])
  x.shape = (n,1)
  return(x,y)

def genxy_intercept(n=2):
  (x, y) = genxy_nointercept(n)
  # add the intercept
  x = np.apply_along_axis(lambda(x1): np.append(np.array([1]), x1), 1, x)
  return(x,y)

def line(x,y):
  a = (y[1]-y[0])/(x[1][1]-x[0][1])
  b = y[0] - a*x[0][1]
  coef = np.append(b,a) 
  # print(coef)
  return(coef)

# just a regression line
def line2(x,y):
  a = (x[0]*y[0] + x[1]*y[1]) / (x[0]*x[0] + x[1]*x[1])
  return(np.array([a]))

def q4(iter=100, genfunc=genxy_nointercept, hypofunc=h2.lm2):
  betas = np.array([])
  f     = np.array([])
  gD    = np.array([])
  xs    = np.array([])
  ncol  = 1
  for i in range(iter):
    (x, y) = genfunc(2)
    beta   = hypofunc(x,y)
    ncol   = beta.size
    gD1    = np.dot(x, beta)
    gD     = np.append(gD, gD1)
    f      = np.append(f, y)
    betas  = np.append(betas, beta)
    xs     = np.append(xs, x)

  betas.shape = (betas.size/ncol, ncol)
  # print(betas)
  betamu  = np.apply_along_axis(np.mean, 0, betas)
  # gbar    = np.mean(gD)
  gbar    = betamu*xs
  var     = np.var(gD)
  bias    = np.mean((gbar - f)**2) 
  return({'betamu':betamu,'betas':betas,'f':f,'xs':xs,'gD':gD,'gbar':gbar,'bias':bias,'var':var})


def q5(iter=100, genfunc=genxy_nointercept, hypofunc=h2.lm2):
  d = q4(iter,genfunc,hypofunc)
  (x,y) = genfunc(iter)
  betamu = d['betamu']
  bias_out = np.mean( (y - betamu*x)**2 )
  return({'bias_out':bias_out,'bias_in':d['bias']})

def q6(iter=100, genfunc=genxy_nointercept, hypofunc=h2.lm2):
  d = q4(iter,genfunc,hypofunc)
  (x,y) = genfunc(iter)
  betas  = d['betas']
  betamu = d['betamu']
  # var in-sample
  var_in = np.mean( (d['gD'] - d['gbar'])**2 )
  # var out-of-sample
  var_out = np.mean( (betas*x - betamu*x)**2 )
  return({'var_out':var_out,'var_in':var_in})


def genxytrans(n=2):
  (x,y) = genxy_nointercept(n)
  xa = np.ones((x.size,1))
  xb = x
  xc = np.apply_along_axis(lambda(x1): np.append(np.array([1]), x1), 1, x) 
  xd = x**2
  xe = np.apply_along_axis(lambda(x1): np.append(np.array([1]), x1**2), 1, x) 
  return({'y':y,'xa':xa,'xb':xb,'xc':xc,'xd':xd,'xe':xe})


def q7(iter=100):
  E = np.zeros((iter,5))
  # iter number of tests
  for i in range(iter):
    # generate the hypothesis
    d  = genxytrans(2) 
    y  = d['y']
    ga = h2.lm2(d['xa'],y)
    gb = h2.lm2(d['xb'],y)
    gc = h2.lm2(d['xc'],y)
    gd = h2.lm2(d['xd'],y)
    ge = h2.lm2(d['xe'],y)

    # test out of sample on hypothesis
    do  = genxytrans(1000) 
    yo  = do['y']
    Ea = np.mean((np.dot(do['xa'],ga) - yo)**2)
    Eb = np.mean((np.dot(do['xb'],gb) - yo)**2)
    Ec = np.mean((np.dot(do['xc'],gc) - yo)**2)
    Ed = np.mean((np.dot(do['xd'],gd) - yo)**2)
    Ee = np.mean((np.dot(do['xe'],ge) - yo)**2)
    E[i] = np.array([Ea,Eb,Ec,Ed,Ee])

  Eavg = np.mean(E,axis=0)
  return({'E':E,'Eavg':Eavg})


"""
VC Dimension

  First, theorem 2.4 (book pg.50) says:

    mH(N) <= sum_{i=0}^{dvc}(N i)
  
  The VC dimension is the order of the polynomial bound on mH(N).

    mH(N) <= N^dvc + 1
      

8. Need to work out the solution
   mH(N+1) = 2mH(N) - (N q)
   mH(1)   = 2
   mH(2)   = 2(2) - (2 q)                  = 4  -  (2 q)
   mH(3)   = 2(4 - (2 q)) - (3 q)          = 8  - 2(2 q) -  (3 q)
   mH(4)   = 2(8 - 2(2 q) - (3 q)) - (4 q) = 16 - 4(2 q) - 2(3 q) - (4 q)
   ...
   mH(N)   = 2^N - (N)(N-2 q) - (N/2)(N-1 q) - (N/2^2)(N q) - ...

           has the form

   mH(N)   = 2^N - N sum( (N q) )

  The question is asking to find VC Dimension of H set whose growth function
  satisfies the question derived from the iterations.

  So when mH(N) <= 2^N, then the dichotomies can be shattered,
  meaning there's a certain break point k.

  We know when q > N,  then (N q) = 0    mH(N) -> 2^N
          when q = N,  then (N q) = 1    mH(N) -> 2^N - N^2
          when q < N,  then (N q) = N^q  mH(N) -> 2^N - N^q = 0

  Thus, the dvc = q


9. [b]  0 <= dvc(intersection H) <= min{ dvc(H) }

"""
